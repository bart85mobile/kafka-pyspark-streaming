{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf4ac83-998d-4a32-a67d-590a8380df2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint = \"<insert-endpoint-here>\"\n",
    "username = \"<insert-username-here>\"\n",
    "password = \"<insert-password-here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e408152-c6f8-45d1-abb9-240196e158f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType\n",
    "\n",
    "# Instantiate\n",
    "spark = SparkSession.builder.appName(\"kafka_spark_poc\").getOrCreate()\n",
    "\n",
    "# Reduce logging verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "jaas_config = f'kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{username}\" password=\"{password}\";'\n",
    "\n",
    "## Kafka configs\n",
    "kafka_source_config = {\n",
    "    \"kafka.sasl.jaas.config\": jaas_config,\n",
    "    \"kafka.bootstrap.servers\" : f\"{endpoint}\",\n",
    "    \"kafka.sasl.mechanism\": \"SCRAM-SHA-256\",\n",
    "    \"kafka.security.protocol\" : \"SASL_SSL\",\n",
    "    \"subscribe\": \"source\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"failOnDataLoss\": \"false\"\n",
    "}\n",
    "kafka_sink_config = {\n",
    "    \"kafka.sasl.jaas.config\": jaas_config,\n",
    "    \"kafka.bootstrap.servers\": f\"{endpoint}\",\n",
    "    \"kafka.sasl.mechanism\": \"SCRAM-SHA-256\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"topic\": \"sink\",\n",
    "    \"checkpointLocation\" : \"./checkpoint.txt\"\n",
    "}\n",
    "\n",
    "## Source Schema\n",
    "df_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    # StructField(\"timestamp\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24088c63-e123-41c4-81b1-ab1df7973903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Read Stream\n",
    "streaming_df = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .options(**kafka_source_config)\\\n",
    "    .load()\n",
    "\n",
    "## Prepare Sink DF\n",
    "sink_df = streaming_df.selectExpr(\"CAST(value AS STRING) as value\")\\\n",
    "    .select(\n",
    "        # Convert the value to a string\n",
    "        F.from_json(F.col(\"value\").cast(\"string\"), df_schema).alias(\"value\")\n",
    "    )\\\n",
    "    .select(\"value.*\")\\\n",
    "    .withColumn(\"is_minor\", F.expr(\"age < 18\"))\\\n",
    "    .select(\n",
    "        # Convert everything back to a json string\n",
    "        F.to_json(\n",
    "            F.struct(\"name\", \"address\", \"age\", \"is_minor\")\n",
    "        ).alias(\"value\")\n",
    "    )\n",
    "\n",
    "## Write Sink DF\n",
    "write = sink_df.writeStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .options(**kafka_sink_config)\\\n",
    "    .start()\n",
    "\n",
    "# Start the streaming application to run until the following happens\n",
    "# 1. Exception in the running program\n",
    "# 2. Manual Interruption\n",
    "write.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Kafka PySpark Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
